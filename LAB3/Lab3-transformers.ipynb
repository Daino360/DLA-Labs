{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c6ae4a",
   "metadata": {},
   "source": [
    "*SOURCES*\n",
    "- Pytorch Documentation\n",
    "- DeepSeek\n",
    "- [Build GPT video](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=3644s)\n",
    "- Hugging Face Documentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e91cdb-4693-4632-b7aa-f37eec027131",
   "metadata": {},
   "source": [
    "## Working with Transformers in the HuggingFace Ecosystem\n",
    "\n",
    "In this laboratory exercise we will learn how to work with the HuggingFace ecosystem to adapt models to new tasks. As you will see, much of what is required is *investigation* into the inner-workings of the HuggingFace abstractions. With a little work, a little trial-and-error, it is fairly easy to get a working adaptation pipeline up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e556105-269f-43e3-8933-227269afb9ea",
   "metadata": {},
   "source": [
    "### Exercise 1: Sentiment Analysis (warm up)\n",
    "\n",
    "In this first exercise we will start from a pre-trained BERT transformer and build up a model able to perform text sentiment analysis. Transformers are complex beasts, so we will build up our pipeline in several explorative and incremental steps.\n",
    "\n",
    "#### Exercise 1.1: Dataset Splits and Pre-trained model\n",
    "There are a many sentiment analysis datasets, but we will use one of the smallest ones available: the [Cornell Rotten Tomatoes movie review dataset](cornell-movie-review-data/rotten_tomatoes), which consists of 5,331 positive and 5,331 negative processed sentences from the Rotten Tomatoes movie reviews.\n",
    "\n",
    "**Your first task**: Load the dataset and figure out what splits are available and how to get them. Spend some time exploring the dataset to see how it is organized. Note that we will be using the [HuggingFace Datasets](https://huggingface.co/docs/datasets/en/index) library for downloading, accessing, splitting, and batching data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15819af4-e850-412b-ab67-61b9b98e3a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: ['train', 'validation', 'test']\n",
      "\n",
      "First training example:\n",
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n",
      "\n",
      "Dataset features:\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n",
      "\n",
      "train set has 8530\n",
      "validation set has 1066\n",
      "test set has 1066\n",
      "\n",
      "Sample of training data:\n",
      "                                                text  label\n",
      "0  the rock is destined to be the 21st century's ...      1\n",
      "1  the gorgeously elaborate continuation of \" the...      1\n",
      "2                     effective but too-tepid biopic      1\n",
      "3  if you sometimes like to go to the movies to h...      1\n",
      "4  emerges as something rare , an issue movie tha...      1\n",
      "\n",
      "Label distribution in train set:\n",
      "Counter({1: 4265, 0: 4265})\n",
      "\n",
      "Label distribution in validation set:\n",
      "Counter({1: 533, 0: 533})\n",
      "\n",
      "Label distribution in test set:\n",
      "Counter({1: 533, 0: 533})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, get_dataset_split_names\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load the rotten_tomatoes dataset\n",
    "# dataset = load_dataset(\"rotbert/rotten_tomatoes\")\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "# Check available splits\n",
    "print(f\"Available splits: {list(dataset.keys())}\")\n",
    "\n",
    "# Examine the first training example\n",
    "print(\"\\nFirst training example:\")\n",
    "print(dataset['train'][0])\n",
    "\n",
    "# Check the features/columns\n",
    "print(\"\\nDataset features:\")\n",
    "print(dataset['train'].features)\n",
    "\n",
    "# Get the number of examples in each split\n",
    "print(\"\")\n",
    "for split in dataset.keys():\n",
    "    print(f\"{split} set has {len(dataset[split])}\")\n",
    "\n",
    "# Convert a small portion to pandas dataframe for easier viewing\n",
    "df = pd.DataFrame(dataset['train'][:5])\n",
    "print(\"\\nSample of training data:\")\n",
    "print(df)\n",
    "\n",
    "for split in dataset.keys():\n",
    "    labels = dataset[split]['label']\n",
    "    print(f\"\\nLabel distribution in {split} set:\")\n",
    "    print(Counter(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f50a0-0af4-4bdd-9a60-bab26f126c14",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: A Pre-trained BERT and Tokenizer\n",
    "\n",
    "The model we will use is a *very* small BERT transformer called [Distilbert](https://huggingface.co/distilbert/distilbert-base-uncased) this model was trained (using self-supervised learning) on the same corpus as BERT but using the full BERT base model as a *teacher*.\n",
    "\n",
    "**Your next task**: Load the Distilbert model and corresponding tokenizer. Use the tokenizer on a few samples from the dataset and pass the tokens through the model to see what outputs are provided. I suggest you use the [`AutoModel`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html) class (and the `from_pretrained()` method) to load the model and `AutoTokenizer` to load the tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eefc651-8551-44c2-8340-37d64660fc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer: <class 'transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFast'>\n",
      "Loaded model: <class 'transformers.models.distilbert.modeling_distilbert.DistilBertModel'>\n",
      "Model configuration:\n",
      "DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "\n",
      "Original samples:\n",
      "1. the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . (Label: 1)\n",
      "2. the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . (Label: 1)\n",
      "3. effective but too-tepid biopic (Label: 1)\n",
      "\n",
      "Tokenized outputs of samples up here:\n",
      "{'input_ids': tensor([[  101,  1996,  2600,  2003, 16036,  2000,  2022,  1996,  7398,  2301,\n",
      "          1005,  1055,  2047,  1000, 16608,  1000,  1998,  2008,  2002,  1005,\n",
      "          1055,  2183,  2000,  2191,  1037, 17624,  2130,  3618,  2084,  7779,\n",
      "         29058,  8625, 13327,  1010,  3744,  1011, 18856, 19513,  3158,  5477,\n",
      "          4168,  2030,  7112, 16562,  2140,  1012,   102,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1996,  9882,  2135,  9603, 13633,  1997,  1000,  1996,  2935,\n",
      "          1997,  1996,  7635,  1000, 11544,  2003,  2061,  4121,  2008,  1037,\n",
      "          5930,  1997,  2616,  3685, 23613,  6235,  2522,  1011,  3213,  1013,\n",
      "          2472,  2848,  4027,  1005,  1055,  4423,  4432,  1997,  1046,  1012,\n",
      "          1054,  1012,  1054,  1012, 23602,  1005,  1055,  2690,  1011,  3011,\n",
      "          1012,   102],\n",
      "        [  101,  4621,  2021,  2205,  1011,  8915, 23267, 16012, 24330,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])}\n",
      "\n",
      "Decoded tokens:\n",
      "\n",
      "Sample 1:\n",
      "Original: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "Token IDs: tensor([  101,  1996,  2600,  2003, 16036,  2000,  2022,  1996,  7398,  2301,\n",
      "         1005,  1055,  2047,  1000, 16608,  1000,  1998,  2008,  2002,  1005,\n",
      "         1055,  2183,  2000,  2191,  1037, 17624,  2130,  3618,  2084,  7779,\n",
      "        29058,  8625, 13327,  1010,  3744,  1011, 18856, 19513,  3158,  5477,\n",
      "         4168,  2030,  7112, 16562,  2140,  1012,   102,     0,     0,     0,\n",
      "            0,     0])\n",
      "Decoded: [CLS] the rock is destined to be the 21st century ' s new \" conan \" and that he ' s going to make a splash even greater than arnold schwarzenegger, jean - claud van damme or steven segal. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Sample 2:\n",
      "Original: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
      "Token IDs: tensor([  101,  1996,  9882,  2135,  9603, 13633,  1997,  1000,  1996,  2935,\n",
      "         1997,  1996,  7635,  1000, 11544,  2003,  2061,  4121,  2008,  1037,\n",
      "         5930,  1997,  2616,  3685, 23613,  6235,  2522,  1011,  3213,  1013,\n",
      "         2472,  2848,  4027,  1005,  1055,  4423,  4432,  1997,  1046,  1012,\n",
      "         1054,  1012,  1054,  1012, 23602,  1005,  1055,  2690,  1011,  3011,\n",
      "         1012,   102])\n",
      "Decoded: [CLS] the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co - writer / director peter jackson ' s expanded vision of j. r. r. tolkien ' s middle - earth. [SEP]\n",
      "\n",
      "Sample 3:\n",
      "Original: effective but too-tepid biopic\n",
      "Token IDs: tensor([  101,  4621,  2021,  2205,  1011,  8915, 23267, 16012, 24330,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "Decoded: [CLS] effective but too - tepid biopic [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Model outputs:\n",
      "Last hidden states shape: torch.Size([3, 52, 768])\n",
      "\n",
      "Model architecture:\n",
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (attention): DistilBertSdpaAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "[CLS] token embeddings shape: torch.Size([3, 768])\n",
      "Sample embedding for first sentence:\n",
      "tensor([-0.0332, -0.0168,  0.0194, -0.0257, -0.1380, -0.3962,  0.3830,  0.5118,\n",
      "         0.0231, -0.0555])...\n",
      "\n",
      "Complex sample processing:\n",
      "Input length: 28 tokens\n",
      "Output shape: torch.Size([1, 28, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Specify the DistilBERT model name\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Let's verify what I loaded\n",
    "print(f\"Loaded tokenizer: {type(tokenizer)}\")\n",
    "print(f\"Loaded model: {type(model)}\")\n",
    "print(f\"Model configuration:\\n{model.config}\")\n",
    "\n",
    "\n",
    "# Get some samples from the dataset\n",
    "samples = dataset['train'][:3]['text']\n",
    "labels = dataset['train'][:3]['label']\n",
    "\n",
    "print(\"\\nOriginal samples:\")\n",
    "for i, sample in enumerate(samples):\n",
    "    print(f\"{i+1}. {sample} (Label: {labels[i]})\")\n",
    "\n",
    "\n",
    "# Tokenize the samples\n",
    "encoded_inputs = tokenizer(samples, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "print(\"\\nTokenized outputs of samples up here:\")\n",
    "print(encoded_inputs)\n",
    "\n",
    "\n",
    "# Decode the tokens to see what the tokenizer did\n",
    "print(\"\\nDecoded tokens:\")\n",
    "for i in range(len(samples)):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Original: {samples[i]}\")\n",
    "    print(f\"Token IDs: {encoded_inputs['input_ids'][i]}\")\n",
    "    print(f\"Decoded: {tokenizer.decode(encoded_inputs['input_ids'][i])}\")\n",
    "\n",
    "\n",
    "# Don't compute gradients\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_inputs)\n",
    "\n",
    "# Examine the outputs\n",
    "print(\"\\nModel outputs:\")\n",
    "print(f\"Last hidden states shape: {outputs.last_hidden_state.shape}\")\n",
    "\n",
    "# ARCHITECTURE\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)\n",
    "\n",
    "\n",
    "# EXTRACTING EMBEDDINGS\n",
    "# Get the embeddings for the [CLS] token (often used for classification)\n",
    "cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "print(f\"\\n[CLS] token embeddings shape: {cls_embeddings.shape}\")\n",
    "print(f\"Sample embedding for first sentence:\\n{cls_embeddings[0][:10]}...\")  # Showing first 10 values\n",
    "\n",
    "\n",
    "# TRYING A COMPLEX SAMPLE\n",
    "complex_sample = \"While the cinematography was stunning and the acting superb, the plot was so convoluted that it ruined the entire movie experience.\"\n",
    "\n",
    "# Tokenize and process\n",
    "inputs = tokenizer(complex_sample, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(f\"\\nComplex sample processing:\")\n",
    "print(f\"Input length: {len(inputs['input_ids'][0])} tokens\")\n",
    "print(f\"Output shape: {outputs.last_hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f6fd4-b80b-466d-b4ff-9aac0492c062",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: A Stable Baseline\n",
    "\n",
    "In this exercise I want you to:\n",
    "1. Use Distilbert as a *feature extractor* to extract representations of the text strings from the dataset splits;\n",
    "2. Train a classifier (your choice, by an SVM from Scikit-learn is an easy choice).\n",
    "3. Evaluate performance on the validation and test splits.\n",
    "\n",
    "These results are our *stable baseline* -- the **starting** point on which we will (hopefully) improve in the next exercise.\n",
    "\n",
    "**Hint**: There are a number of ways to implement the feature extractor, but probably the best is to use a [feature extraction `pipeline`](https://huggingface.co/tasks/feature-extraction). You will need to interpret the output of the pipeline and extract only the `[CLS]` token from the *last* transformer layer. *How can you figure out which output that is?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb961ab-aa24-4d5c-86a1-c9b4237b40fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize feature extraction pipeline\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "feature_extractor = pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=checkpoint,\n",
    "    tokenizer=\"distilbert-base-uncased\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be934657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cls_embeddings_batch(texts, tokenizer, model, batch_size=32, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embeds = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "            embeddings.append(cls_embeds.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b07cd971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting training features...\n",
      "\n",
      "Extracting validation features...\n",
      "\n",
      "Extracting test features...\n",
      "\n",
      "Train embeddings shape: (8530, 768)\n",
      "Validation embeddings shape: (1066, 768)\n",
      "Test embeddings shape: (1066, 768)\n"
     ]
    }
   ],
   "source": [
    "# Extract features for each split\n",
    "print(\"Extracting training features...\")\n",
    "train_embeddings = extract_cls_embeddings_batch(dataset['train']['text'], tokenizer, model)\n",
    "train_labels = np.array(dataset['train']['label'])\n",
    "\n",
    "print(\"\\nExtracting validation features...\")\n",
    "val_embeddings = extract_cls_embeddings_batch(dataset['validation']['text'], tokenizer, model)\n",
    "val_labels = np.array(dataset['validation']['label'])\n",
    "\n",
    "print(\"\\nExtracting test features...\")\n",
    "test_embeddings = extract_cls_embeddings_batch(dataset['test']['text'], tokenizer, model)\n",
    "test_labels = np.array(dataset['test']['label'])\n",
    "\n",
    "# Check shapes\n",
    "print(f\"\\nTrain embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"Validation embeddings shape: {val_embeddings.shape}\")\n",
    "print(f\"Test embeddings shape: {test_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b742c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.84      0.83       533\n",
      "    positive       0.84      0.80      0.82       533\n",
      "\n",
      "    accuracy                           0.82      1066\n",
      "   macro avg       0.82      0.82      0.82      1066\n",
      "weighted avg       0.82      0.82      0.82      1066\n",
      "\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.81      0.80       533\n",
      "    positive       0.81      0.78      0.80       533\n",
      "\n",
      "    accuracy                           0.80      1066\n",
      "   macro avg       0.80      0.80      0.80      1066\n",
      "weighted avg       0.80      0.80      0.80      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize and train SVM\n",
    "svm = LinearSVC(random_state=42, max_iter=10000)\n",
    "svm.fit(train_embeddings, train_labels)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_preds = svm.predict(val_embeddings)\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=['negative', 'positive']))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_preds = svm.predict(test_embeddings)\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19e8bea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Validation Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.86      0.83       533\n",
      "    positive       0.85      0.80      0.83       533\n",
      "\n",
      "    accuracy                           0.83      1066\n",
      "   macro avg       0.83      0.83      0.83      1066\n",
      "weighted avg       0.83      0.83      0.83      1066\n",
      "\n",
      "\n",
      "Logistic Regression Test Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.81      0.80       533\n",
      "    positive       0.81      0.79      0.80       533\n",
      "\n",
      "    accuracy                           0.80      1066\n",
      "   macro avg       0.80      0.80      0.80      1066\n",
      "weighted avg       0.80      0.80      0.80      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train logistic regression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(train_embeddings, train_labels)\n",
    "\n",
    "# Evaluate\n",
    "lr_val_preds = lr.predict(val_embeddings)\n",
    "print(\"\\nLogistic Regression Validation Performance:\")\n",
    "print(classification_report(val_labels, lr_val_preds, target_names=['negative', 'positive']))\n",
    "\n",
    "lr_test_preds = lr.predict(test_embeddings)\n",
    "print(\"\\nLogistic Regression Test Performance:\")\n",
    "print(classification_report(test_labels, lr_test_preds, target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2107e109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Predictions:\n",
      "\n",
      "Text: exposing the ways we fool ourselves is one hour photo's real strength .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: this kind of hands-on storytelling is ultimately what makes shanghai ghetto move beyond a good , dry , reliable textbook and what allows it to rank with its worthy predecessors .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: what's so striking about jolie's performance is that she never lets her character become a caricature -- not even with that radioactive hair .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Get some example predictions\n",
    "sample_indices = [10, 20, 30]  # Random examples\n",
    "sample_texts = [dataset['test']['text'][i] for i in sample_indices]\n",
    "sample_true = [dataset['test']['label'][i] for i in sample_indices]\n",
    "sample_preds = test_preds[sample_indices]\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "for text, true_label, pred_label in zip(sample_texts, sample_true, sample_preds):\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"True: {'positive' if true_label else 'negative'}\")\n",
    "    print(f\"Pred: {'positive' if pred_label else 'negative'}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37141d1b-935b-425c-804c-b9b487853791",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 2: Fine-tuning Distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a53f64-0238-42f2-bc20-86e51c77d2e5",
   "metadata": {},
   "source": [
    "In this exercise we will fine-tune the Distilbert model to (hopefully) improve sentiment analysis performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392b1ed-597b-4a92-90fc-10eb11eac515",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Token Preprocessing\n",
    "\n",
    "The first thing we need to do is *tokenize* our dataset splits. Our current datasets return a dictionary with *strings*, but we want *input token ids* (i.e. the output of the tokenizer). This is easy enough to do my hand, but the HugginFace `Dataset` class provides convenient, efficient, and *lazy* methods. See the documentation for [`Dataset.map`](https://huggingface.co/docs/datasets/v3.5.0/en/package_reference/main_classes#datasets.Dataset.map).\n",
    "\n",
    "**Tip**: Verify that your new datasets are returning for every element: `text`, `label`, `intput_ids`, and `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e6e2a95-5b08-4f81-b824-59a0fb3404e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, get_dataset_split_names\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the text examples and return relevant fields\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',  # Pad to max sequence length\n",
    "        truncation=True,       # Truncate to max length\n",
    "        max_length=128,        # Set maximum sequence length\n",
    "        return_tensors=None    # Return as plain lists (not tensors)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e56d865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all splits\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,          # Process in batches for efficiency\n",
    "    batch_size=32,         # Adjust based on your memory\n",
    "    remove_columns=['text'] # Remove original text column (we keep tokenized version)\n",
    ")\n",
    "\n",
    "# Verify the new structure\n",
    "print(\"Tokenized dataset structure:\")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bc4547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed dataset features:\n",
      "{'label': ClassLabel(names=['neg', 'pos'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "\n",
      "Sample processed example:\n",
      "Input IDs length: 128\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Check the features of the processed dataset\n",
    "print(\"\\nProcessed dataset features:\")\n",
    "print(tokenized_datasets['train'].features)\n",
    "\n",
    "# Examine a single example\n",
    "sample = tokenized_datasets['train'][0]\n",
    "print(\"\\nSample processed example:\")\n",
    "print(f\"Input IDs length: {len(sample['input_ids'])}\")\n",
    "print(f\"Attention mask: {sample['attention_mask'][:10]}...\")  # Show first 10 positions\n",
    "print(f\"Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cde1e085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded sample:\n",
      "[CLS] the rock is destined to be the 21st century ' s new \" conan \" and that he ' s going to make a splash even greater than arnold schwarzenegger, jean - claud van damme or steven segal. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Original text:\n",
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n"
     ]
    }
   ],
   "source": [
    "# Decode a sample to verify tokenization\n",
    "sample_ids = tokenized_datasets['train'][0]['input_ids']\n",
    "print(\"\\nDecoded sample:\")\n",
    "print(tokenizer.decode(sample_ids))\n",
    "\n",
    "# Compare with original\n",
    "original_text = dataset['train'][0]['text']\n",
    "print(\"\\nOriginal text:\")\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60cb3fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split train has all required fields\n",
      "Split validation has all required fields\n",
      "Split test has all required fields\n"
     ]
    }
   ],
   "source": [
    "# Dataset Format Verification\n",
    "required_fields = {'input_ids', 'attention_mask', 'label'}\n",
    "\n",
    "for split in tokenized_datasets.keys():\n",
    "    current_fields = set(tokenized_datasets[split].features.keys())\n",
    "    missing = required_fields - current_fields\n",
    "    if missing:\n",
    "        print(f\"Warning: Split {split} missing fields: {missing}\")\n",
    "    else:\n",
    "        print(f\"Split {split} has all required fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b978065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train set token statistics:\n",
      "Average length: 128.0\n",
      "Max length: 128\n",
      "Min length: 128\n",
      "Padded length: 128\n",
      "\n",
      "validation set token statistics:\n",
      "Average length: 128.0\n",
      "Max length: 128\n",
      "Min length: 128\n",
      "Padded length: 128\n",
      "\n",
      "test set token statistics:\n",
      "Average length: 128.0\n",
      "Max length: 128\n",
      "Min length: 128\n",
      "Padded length: 128\n"
     ]
    }
   ],
   "source": [
    "# Dataset Statistics\n",
    "import numpy as np\n",
    "\n",
    "for split in tokenized_datasets.keys():\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_datasets[split]]\n",
    "    print(f\"\\n{split} set token statistics:\")\n",
    "    print(f\"Average length: {np.mean(lengths):.1f}\")\n",
    "    print(f\"Max length: {max(lengths)}\")\n",
    "    print(f\"Min length: {min(lengths)}\")\n",
    "    print(f\"Padded length: {len(tokenized_datasets[split][0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0aea266a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c3f4fba1944bfc843776614bdecb38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df0b58cfe4e4d5ab80be2c9422189bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14d49223c2142cfbdc078dd3ebcb5ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save processed datasets\n",
    "tokenized_datasets.save_to_disk(\"tokenized_rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c39a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load later:\n",
    "from datasets import load_from_disk\n",
    "tokenized_datasets = load_from_disk(\"tokenized_rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a80de2-83c9-4c12-af4e-34babe23ffd1",
   "metadata": {},
   "source": [
    "#### Exercise 2.2: Setting up the Model to be Fine-tuned\n",
    "\n",
    "In this exercise we need to prepare the base Distilbert model for fine-tuning for a *sequence classification task*. This means, at the very least, appending a new, randomly-initialized classification head connected to the `[CLS]` token of the last transformer layer. Luckily, HuggingFace already provides an `AutoModel` for just this type of instantiation: [`AutoModelForSequenceClassification`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification). You will want you instantiate one of these for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6327d73-3b71-478a-9932-bb062a650c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the Model with Classification Head\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load DistilBERT with a sequence classification head\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",  # Base pre-trained model\n",
    "    num_labels=2,               # Number of output labels (positive/negative)\n",
    "    id2label={0: \"negative\", 1: \"positive\"},  # Optional: label mapping\n",
    "    label2id={\"negative\": 0, \"positive\": 1}   # Optional: reverse mapping\n",
    ")\n",
    "\n",
    "# Verify the model architecture\n",
    "print(\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3242483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification head:\n",
      "Linear(in_features=768, out_features=2, bias=True)\n",
      "\n",
      "Input dimension to classifier: 768\n",
      "Output dimension: 2\n"
     ]
    }
   ],
   "source": [
    "# Examine the Classification Head\n",
    "\n",
    "# Check the classification head\n",
    "print(\"\\nClassification head:\")\n",
    "print(model.classifier)\n",
    "\n",
    "# Check the dimensions\n",
    "print(f\"\\nInput dimension to classifier: {model.classifier.in_features}\")\n",
    "print(f\"Output dimension: {model.classifier.out_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9394a043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model configuration:\n",
      "DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "\n",
      "Hidden size: 768\n",
      "Number of labels: 2\n"
     ]
    }
   ],
   "source": [
    "# Model Configuration\n",
    "\n",
    "# Print model configuration\n",
    "print(\"\\nModel configuration:\")\n",
    "print(model.config)\n",
    "\n",
    "# Important configuration parameters\n",
    "print(f\"\\nHidden size: {model.config.hidden_size}\")\n",
    "print(f\"Number of labels: {model.config.num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ca2290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model outputs:\n",
      "Logits shape: torch.Size([1, 2])\n",
      "Sample logits: tensor([[-0.0224, -0.0162]])\n"
     ]
    }
   ],
   "source": [
    "# Verify Forward Pass\n",
    "import torch\n",
    "\n",
    "# Get a sample from our tokenized dataset\n",
    "sample = tokenized_datasets['train'][0]\n",
    "input_ids = torch.tensor([sample['input_ids']])\n",
    "attention_mask = torch.tensor([sample['attention_mask']])\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Examine outputs\n",
    "print(\"\\nModel outputs:\")\n",
    "print(f\"Logits shape: {outputs.logits.shape}\")\n",
    "print(f\"Sample logits: {outputs.logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee702315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model moved to: cuda\n",
      "Outputs generated on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check Device Compatibility\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"\\nModel moved to: {device}\")\n",
    "\n",
    "# Verify device placement\n",
    "sample_input = input_ids.to(device)\n",
    "sample_attention = attention_mask.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=sample_input, attention_mask=sample_attention)\n",
    "print(f\"Outputs generated on: {outputs.logits.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6822063a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model in training mode: False\n",
      "\n",
      "Parameter gradients:\n",
      "distilbert.embeddings.word_embeddings.weight: requires grad (trainable)\n",
      "distilbert.embeddings.position_embeddings.weight: requires grad (trainable)\n",
      "distilbert.embeddings.LayerNorm.weight: requires grad (trainable)\n",
      "distilbert.embeddings.LayerNorm.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.attention.q_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.attention.q_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.attention.v_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.attention.v_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.attention.q_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.attention.q_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.attention.k_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.attention.k_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.attention.v_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.attention.v_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.attention.out_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.attention.out_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.sa_layer_norm.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.sa_layer_norm.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.ffn.lin1.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.ffn.lin1.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.ffn.lin2.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.ffn.lin2.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.output_layer_norm.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.1.output_layer_norm.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.attention.q_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.attention.q_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.attention.k_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.attention.k_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.attention.v_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.attention.v_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.attention.out_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.attention.out_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.sa_layer_norm.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.sa_layer_norm.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.ffn.lin1.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.ffn.lin1.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.ffn.lin2.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.ffn.lin2.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.output_layer_norm.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.2.output_layer_norm.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.attention.q_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.attention.q_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.attention.k_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.attention.k_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.attention.v_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.attention.v_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.attention.out_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.attention.out_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.sa_layer_norm.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.sa_layer_norm.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.ffn.lin1.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.ffn.lin1.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.ffn.lin2.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.ffn.lin2.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.output_layer_norm.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.3.output_layer_norm.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.attention.q_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.attention.q_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.attention.k_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.attention.k_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.attention.v_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.attention.v_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.attention.out_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.attention.out_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.sa_layer_norm.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.sa_layer_norm.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.ffn.lin1.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.ffn.lin1.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.ffn.lin2.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.ffn.lin2.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.output_layer_norm.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.4.output_layer_norm.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.attention.q_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.attention.q_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.attention.k_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.attention.k_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.attention.v_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.attention.v_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.attention.out_lin.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.attention.out_lin.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.sa_layer_norm.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.sa_layer_norm.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.ffn.lin1.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.ffn.lin1.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.ffn.lin2.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.ffn.lin2.bias: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.output_layer_norm.weight: requires grad (trainable)\n",
      "distilbert.transformer.layer.5.output_layer_norm.bias: requires grad (trainable)\n",
      "pre_classifier.weight: requires grad (trainable)\n",
      "pre_classifier.bias: requires grad (trainable)\n",
      "classifier.weight: requires grad (trainable)\n",
      "classifier.bias: requires grad (trainable)\n"
     ]
    }
   ],
   "source": [
    "# Verify Training Readiness\n",
    "\n",
    "# Check training mode\n",
    "print(f\"\\nModel in training mode: {model.training}\")\n",
    "\n",
    "# Check parameter requires_grad\n",
    "print(\"\\nParameter gradients:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: requires grad (trainable)\")\n",
    "    else:\n",
    "        print(f\"{name}: frozen (not trainable)\")\n",
    "\n",
    "# By default, all parameters should be trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109f7bd-955f-4fc4-bc3e-75bd99a17adf",
   "metadata": {},
   "source": [
    "#### Exercise 2.3: Fine-tuning Distilbert\n",
    "\n",
    "Finally. In this exercise you should use a HuggingFace [`Trainer`](https://huggingface.co/docs/transformers/main/en/trainer) to fine-tune your model on the Rotten Tomatoes training split. Setting up the trainer will involve (at least):\n",
    "\n",
    "\n",
    "1. Instantiating a [`DataCollatorWithPadding`](https://huggingface.co/docs/transformers/en/main_classes/data_collator) object which is what *actually* does your batch construction (by padding all sequences to the same length).\n",
    "2. Writing an *evaluation function* that will measure the classification accuracy. This function takes a single argument which is a tuple containing `(logits, labels)` which you should use to compute classification accuracy (and maybe other metrics like F1 score, precision, recall) and return a `dict` with these metrics.  \n",
    "3. Instantiating a [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.51.1/en/main_classes/trainer#transformers.TrainingArguments) object using some reasonable defaults.\n",
    "4. Instantiating a `Trainer` object using your train and validation splits, you data collator, and function to compute performance metrics.\n",
    "5. Calling `trainer.train()`, waiting, waiting some more, and then calling `trainer.evaluate()` to see how it did.\n",
    "\n",
    "**Tip**: When prototyping this laboratory I discovered the HuggingFace [Evaluate library](https://huggingface.co/docs/evaluate/en/index) which provides evaluation metrics. However I found it to have insufferable layers of abstraction and getting actual metrics computed. I suggest just using the Scikit-learn metrics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab17178d-5028-47e3-af97-953e8de5aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collated batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Input IDs shape: torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding='longest',  # Pad to longest in batch\n",
    "    return_tensors='pt' # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Test the collator with a sample batch\n",
    "sample_batch = [tokenized_datasets['train'][i] for i in range(2)]\n",
    "collated = data_collator(sample_batch)\n",
    "print(\"Collated batch keys:\", collated.keys())\n",
    "print(\"Input IDs shape:\", collated['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b163bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for evaluation\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='macro'),\n",
    "        'precision': precision_score(labels, predictions, average='macro'),\n",
    "        'recall': recall_score(labels, predictions, average='macro')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ac907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Set training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    eval_strategy='epoch',           # Evaluate after each epoch\n",
    "    save_strategy='epoch',           # Save after each epoch\n",
    "    learning_rate=3e-5,              # Learning rate\n",
    "    per_device_train_batch_size=8,   # Batch size\n",
    "    per_device_eval_batch_size=16,   # Eval batch size\n",
    "    num_train_epochs=5,              # Number of epochs\n",
    "    weight_decay=0.02,               # Weight decay\n",
    "    load_best_model_at_end=True,     # Load best model at end\n",
    "    metric_for_best_model='f1',      # Use F1 to select best model\n",
    "    logging_dir='./logs',            # Logging directory\n",
    "    logging_steps=50,                # Log every 50 steps\n",
    "    report_to='none',                # Disable external logging\n",
    "    gradient_accumulation_steps=2,   # Effective batch size of 16\n",
    "    fp16=True,                       # Mixed precision training\n",
    "    gradient_checkpointing=True,     # Memory optimization\n",
    "    warmup_ratio=0.1,                # Learning rate warmup\n",
    "    lr_scheduler_type=\"cosine\",      # Learning rate decay\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8486e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer, # tokenizer is deprecated, used processing_class instead\n",
    "    callbacks=[EarlyStoppingCallback(\n",
    "    early_stopping_patience=2,\n",
    "    early_stopping_threshold=0.001\n",
    "    )]\n",
    "    \n",
    ")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2f2878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/miniconda3/envs/transformers/lib/python3.13/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1602' max='2670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1602/2670 23:21 < 15:35, 1.14 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.403002</td>\n",
       "      <td>0.822702</td>\n",
       "      <td>0.822633</td>\n",
       "      <td>0.823203</td>\n",
       "      <td>0.822702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.399947</td>\n",
       "      <td>0.818949</td>\n",
       "      <td>0.818879</td>\n",
       "      <td>0.819445</td>\n",
       "      <td>0.818949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.432200</td>\n",
       "      <td>0.400174</td>\n",
       "      <td>0.821764</td>\n",
       "      <td>0.821763</td>\n",
       "      <td>0.821768</td>\n",
       "      <td>0.821764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/miniconda3/envs/transformers/lib/python3.13/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/stefano/miniconda3/envs/transformers/lib/python3.13/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics:\n",
      "eval_loss: 0.4030\n",
      "eval_accuracy: 0.8227\n",
      "eval_f1: 0.8226\n",
      "eval_precision: 0.8232\n",
      "eval_recall: 0.8227\n",
      "eval_runtime: 45.7464\n",
      "eval_samples_per_second: 23.3020\n",
      "eval_steps_per_second: 1.4650\n",
      "epoch: 3.0000\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "Test metrics:\n",
      "eval_loss: 0.4420\n",
      "eval_accuracy: 0.7974\n",
      "eval_f1: 0.7973\n",
      "eval_precision: 0.7976\n",
      "eval_recall: 0.7974\n",
      "eval_runtime: 46.4760\n",
      "eval_samples_per_second: 22.9370\n",
      "eval_steps_per_second: 1.4420\n",
      "epoch: 3.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "val_metrics = trainer.evaluate()\n",
    "print(\"\\nValidation metrics:\")\n",
    "for k, v in val_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_metrics = trainer.evaluate(tokenized_datasets['test'])\n",
    "print(\"\\nTest metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c031d6",
   "metadata": {},
   "source": [
    "**Analisi Risultati**:\n",
    "\n",
    "Il modello è stato addestrato per 3 epoche con risultati di validazione stabili, mostrando già nella prima epoca un'accuratezza elevata (82.3%), le metriche di validazione (accuracy, F1, precision, recall) si mantengono tutte sopra l’82%, suggerendo una buona generalizzazione.\n",
    "\n",
    "La crescita delle loss indica un possibile overfitting iniziale e la differenza tra training e validation loss rimane contenuta, suggerendo che il modello non sta memorizzando eccessivamente.\n",
    "\n",
    "L’accuratezza sul test set (79.7%) è leggermente inferiore rispetto alla validazione, ma ancora vicina, confermando però delle buone prestazioni. Le metriche test F1, precision e recall sono tutte bilanciate (~79.7%), pertanto che il modello non favorisce una classe rispetto all’altra.\n",
    "\n",
    "In sintesi, il modello fine-tuned si comporta bene su sentiment analysis, con generalizzazione accettabile sul test set, anche se non mostra grandi miglioramenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a661a028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_distilbert/tokenizer_config.json',\n",
       " './fine_tuned_distilbert/special_tokens_map.json',\n",
       " './fine_tuned_distilbert/vocab.txt',\n",
       " './fine_tuned_distilbert/added_tokens.json',\n",
       " './fine_tuned_distilbert/tokenizer.json')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best model\n",
    "trainer.save_model('./fine_tuned_distilbert')\n",
    "\n",
    "# Save tokenizer as well\n",
    "tokenizer.save_pretrained('./fine_tuned_distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7ac330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./fine_tuned_distilbert')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('./fine_tuned_distilbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "acf9706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: consistently clever and suspenseful .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: it's like a \" big chill \" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with tremendous skill .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: red dragon \" never cuts corners .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: fresnadillo has something serious to say about the ways in which extravagant chance can distort our perspective and throw us off the path of good sense .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: throws in enough clever and unexpected twists to make the formula feel fresh .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: weighty and ponderous but every bit as filling as the treat of the title .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: a real audience-pleaser that will strike a chord with anyone who's ever waited in a doctor's office , emergency room , hospital bed or insurance company office .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: generates an enormous feeling of empathy for its characters .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: exposing the ways we fool ourselves is one hour photo's real strength .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: it's up to you to decide whether to admire these people's dedication to their cause or be repelled by their dogmatism , manipulativeness and narrow , fearful view of american life .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: mostly , [goldbacher] just lets her complicated characters be unruly , confusing and , through it all , human .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: . . . quite good at providing some good old fashioned spooks .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: at its worst , the movie is pretty diverting ; the pity is that it rarely achieves its best .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: scherfig's light-hearted profile of emotional desperation is achingly honest and delightfully cheeky .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: a journey spanning nearly three decades of bittersweet camaraderie and history , in which we feel that we truly know what makes holly and marina tick , and our hearts go out to them as both continue to negotiate their imperfect , love-hate relationship .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: the wonderfully lush morvern callar is pure punk existentialism , and ms . ramsay and her co-writer , liana dognini , have dramatized the alan warner novel , which itself felt like an answer to irvine welsh's book trainspotting .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: as it turns out , you can go home again .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: you've already seen city by the sea under a variety of titles , but it's worth yet another visit .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: this kind of hands-on storytelling is ultimately what makes shanghai ghetto move beyond a good , dry , reliable textbook and what allows it to rank with its worthy predecessors .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: making such a tragedy the backdrop to a love story risks trivializing it , though chouraqui no doubt intended the film to affirm love's power to help people endure almost unimaginable horror .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: grown-up quibbles are beside the point here . the little girls understand , and mccracken knows that's all that matters .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: a powerful , chilling , and affecting study of one man's dying fall .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: this is a fascinating film because there is no clear-cut hero and no all-out villain .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: a dreadful day in irish history is given passionate , if somewhat flawed , treatment .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: . . . a good film that must have baffled the folks in the marketing department .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: . . . is funny in the way that makes you ache with sadness ( the way chekhov is funny ) , profound without ever being self-important , warm without ever succumbing to sentimentality .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: devotees of star trek ii : the wrath of khan will feel a nagging sense of deja vu , and the grandeur of the best next generation episodes is lacking .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: a soul-stirring documentary about the israeli/palestinian conflict as revealed through the eyes of some children who remain curious about each other against all odds .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: what's so striking about jolie's performance is that she never lets her character become a caricature -- not even with that radioactive hair .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: the main story . . . is compelling enough , but it's difficult to shrug off the annoyance of that chatty fish .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: the performances are immaculate , with roussillon providing comic relief .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: kinnear . . . gives his best screen performance with an oddly winning portrayal of one of life's ultimate losers .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: hugh grant , who has a good line in charm , has never been more charming than in about a boy .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: there's a lot of tooth in roger dodger . but what's nice is that there's a casual intelligence that permeates the script .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: reminiscent of alfred hitchcock's thrillers , most of the scary parts in 'signs' occur while waiting for things to happen .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: one of the best looking and stylish animated movies in quite a while . . .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: its use of the thriller form to examine the labyrinthine ways in which people's lives cross and change , buffeted by events seemingly out of their control , is intriguing , provocative stuff .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: denver should not get the first and last look at one of the most triumphant performances of vanessa redgrave's career . it deserves to be seen everywhere .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: you needn't be steeped in '50s sociology , pop culture or movie lore to appreciate the emotional depth of haynes' work . though haynes' style apes films from the period . . . its message is not rooted in that decade .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: waiting for godard can be fruitful : 'in praise of love' is the director's epitaph for himself .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: a gangster movie with the capacity to surprise .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: the film has a laundry list of minor shortcomings , but the numerous scenes of gory mayhem are worth the price of admission . . . if \" gory mayhem \" is your idea of a good time .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: if not a home run , then at least a solid base hit .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: goldmember is funny enough to justify the embarrassment of bringing a barf bag to the moviehouse .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: . . . a fairly disposable yet still entertaining b picture .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: it may not be particularly innovative , but the film's crisp , unaffected style and air of gentle longing make it unexpectedly rewarding .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: the film truly does rescue [the funk brothers] from motown's shadows . it's about time .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: drawing on an irresistible , languid romanticism , byler reveals the ways in which a sultry evening or a beer-fueled afternoon in the sun can inspire even the most retiring heart to venture forth .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: works because we're never sure if ohlinger's on the level or merely a dying , delusional man trying to get into the history books before he croaks .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: [scherfig] has made a movie that will leave you wondering about the characters' lives after the clever credits roll .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: a heady , biting , be-bop ride through nighttime manhattan , a loquacious videologue of the modern male and the lengths to which he'll go to weave a protective cocoon around his own ego .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: skin of man gets a few cheap shocks from its kids-in-peril theatrics , but it also taps into the primal fears of young people trying to cope with the mysterious and brutal nature of adults .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: the piano teacher is not an easy film . it forces you to watch people doing unpleasant things to each other and themselves , and it maintains a cool distance from its material that is deliberately unsettling .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: as refreshing as a drink from a woodland stream .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: williams absolutely nails sy's queasy infatuation and overall strangeness .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: can i admit xxx is as deep as a petri dish and as well-characterized as a telephone book but still say it was a guilty pleasure ?\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: while it's nothing we haven't seen before from murphy , i spy is still fun and enjoyable and so aggressively silly that it's more than a worthwhile effort .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: by the time it ends in a rush of sequins , flashbulbs , blaring brass and back-stabbing babes , it has said plenty about how show business has infiltrated every corner of society -- and not always for the better .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: an intimate contemplation of two marvelously messy lives .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: rarely has skin looked as beautiful , desirable , even delectable , as it does in trouble every day .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: this is one of those rare docs that paints a grand picture of an era and makes the journey feel like a party .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: poignant if familiar story of a young person suspended between two cultures .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: a metaphor for a modern-day urban china searching for its identity .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: for all its brooding quality , ash wednesday is suspenseful and ultimately unpredictable , with a sterling ensemble cast .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: an odd drama set in the world of lingerie models and bar dancers in the midwest that held my interest precisely because it didn't try to .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: the film feels uncomfortably real , its language and locations bearing the unmistakable stamp of authority .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: despite its faults , gangs excels in spectacle and pacing .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: entertaining despite its one-joke premise with the thesis that women from venus and men from mars can indeed get together .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: a tightly directed , highly professional film that's old-fashioned in all the best possible ways .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: it's dark but has wonderfully funny moments ; you care about the characters ; and the action and special effects are first-rate .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: in visual fertility treasure planet rivals the top japanese animations of recent vintage .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: enormously enjoyable , high-adrenaline documentary .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: buy is an accomplished actress , and this is a big , juicy role .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: it works its magic with such exuberance and passion that the film's length becomes a part of its fun .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: beautifully crafted and brutally honest , promises offers an unexpected window into the complexities of the middle east struggle and into the humanity of its people .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: an old-fashioned but emotionally stirring adventure tale of the kind they rarely make anymore .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: charlotte sometimes is a gem . it's always enthralling .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: in my opinion , analyze that is not as funny or entertaining as analyze this , but it is a respectable sequel .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: a remarkable film by bernard rose .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: zhuangzhuang creates delicate balance of style , text , and subtext that's so simple and precise that anything discordant would topple the balance , but against all odds , nothing does .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: a much more successful translation than its most famous previous film adaptation , writer-director anthony friedman's similarly updated 1970 british production .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: an original and highly cerebral examination of the psychopathic mind\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: michel piccoli's moving performance is this films reason for being .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: a captivating and intimate study about dying and loving . . .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: this is an elegantly balanced movie -- every member of the ensemble has something fascinating to do -- that doesn't reveal even a hint of artifice .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: [grant] goes beyond his usual fluttering and stammering and captures the soul of a man in pain who gradually comes to recognize it and deal with it .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: a high-spirited buddy movie about the reunion of berlin anarchists who face arrest 15 years after their crime .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: about the best thing you could say about narc is that it's a rock-solid little genre picture . whether you like it or not is basically a matter of taste .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: an involving , inspirational drama that sometimes falls prey to its sob-story trappings .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: some of the most inventive silliness you are likely to witness in a movie theatre for some time .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: canadian filmmaker gary burns' inventive and mordantly humorous take on the soullessness of work in the city .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: a rollicking ride , with jaw-dropping action sequences , striking villains , a gorgeous color palette , astounding technology , stirring music and a boffo last hour that leads up to a strangely sinister happy ending .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: everyone's insecure in lovely and amazing , a poignant and wryly amusing film about mothers , daughters and their relationships .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: the closest thing to the experience of space travel\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: full of surprises .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: connoisseurs of chinese film will be pleased to discover that tian's meticulous talent has not withered during his enforced hiatus .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: if you can push on through the slow spots , you'll be rewarded with some fine acting .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: an unusually dry-eyed , even analytical approach to material that is generally played for maximum moisture .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for test set\n",
    "predictions = trainer.predict(tokenized_datasets['test'])\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Show some examples\n",
    "sample_indices = [10, 20, 30, 40, 50]\n",
    "for idx in  range(1,100): #sample_indices:\n",
    "    text = dataset['test']['text'][idx]\n",
    "    true_label = dataset['test']['label'][idx]\n",
    "    pred_label = pred_labels[idx]\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"True: {'positive' if true_label else 'negative'}\")\n",
    "    print(f\"Pred: {'positive' if pred_label else 'negative'}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8376de-8554-4a13-aac3-59257f3eb3fd",
   "metadata": {},
   "source": [
    "-----\n",
    "### Exercise 3: Choose at Least One\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55cf4d-e64b-47fc-b8d5-37288b72d90d",
   "metadata": {},
   "source": [
    "#### Exercise 3.1: Efficient Fine-tuning for Sentiment Analysis (easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f183856-1111-4fe9-81f1-691fe7c1b706",
   "metadata": {},
   "source": [
    "In Exercise 2 we fine-tuned the *entire* Distilbert model on Rotten Tomatoes. This is expensive, even for a small model. Find an *efficient* way to fine-tune Distilbert on the Rotten Tomatoes dataset (or some other dataset).\n",
    "\n",
    "**Hint**: You could check out the [HuggingFace PEFT library](https://huggingface.co/docs/peft/en/index) for some state-of-the-art approaches that should \"just work\". How else might you go about making fine-tuning more efficient without having to change your training pipeline from above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ea6bca5-9b36-424e-898c-52c0777eae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 771,842 || all params: 67,713,028 || trainable%: 1.1399\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"}\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=6,  # Slightly higher rank than before\n",
    "    lora_alpha=12,\n",
    "    target_modules=[\"q_lin\", \"v_lin\", \"out_lin\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"lora_only\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    #fan_in_fan_out=True,  # Fixes gradient flow\n",
    "    modules_to_save=[\"classifier\"]  # Ensure classifier gets gradients\n",
    ")\n",
    "\n",
    "# Wrap model with LoRA\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Check trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "# Typically shows <1% of parameters are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e133af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load and tokenize data (same as before)\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128  # Reduced from default 512 if possible\n",
    "    )\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./peft-lora\",\n",
    "    learning_rate=8e-5,  # Adjusted learning rate\n",
    "    per_device_train_batch_size=6,  # Balanced batch size\n",
    "    per_device_eval_batch_size=12,\n",
    "    gradient_accumulation_steps=3,\n",
    "    num_train_epochs=12,  # More epochs with early stopping\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=50,\n",
    "    warmup_steps=100,  # Explicit warmup\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_pin_memory=True,  # Faster data loading\n",
    "    dataloader_num_workers=2  # If you have multiple CPU cores\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "        #GradientDebugCallback()  # For monitoring\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fd7136db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/miniconda3/envs/transformers/lib/python3.13/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3318' max='5688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3318/5688 55:49 < 39:53, 0.99 it/s, Epoch 7/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.487400</td>\n",
       "      <td>0.455879</td>\n",
       "      <td>0.802064</td>\n",
       "      <td>0.801937</td>\n",
       "      <td>0.802841</td>\n",
       "      <td>0.802064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.451000</td>\n",
       "      <td>0.420344</td>\n",
       "      <td>0.818949</td>\n",
       "      <td>0.818865</td>\n",
       "      <td>0.819544</td>\n",
       "      <td>0.818949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.454500</td>\n",
       "      <td>0.408677</td>\n",
       "      <td>0.819887</td>\n",
       "      <td>0.819865</td>\n",
       "      <td>0.820050</td>\n",
       "      <td>0.819887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.435800</td>\n",
       "      <td>0.405964</td>\n",
       "      <td>0.823640</td>\n",
       "      <td>0.823640</td>\n",
       "      <td>0.823640</td>\n",
       "      <td>0.823640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.454600</td>\n",
       "      <td>0.401385</td>\n",
       "      <td>0.821764</td>\n",
       "      <td>0.821761</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>0.821764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.418300</td>\n",
       "      <td>0.403893</td>\n",
       "      <td>0.810507</td>\n",
       "      <td>0.809821</td>\n",
       "      <td>0.815049</td>\n",
       "      <td>0.810507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.431700</td>\n",
       "      <td>0.404573</td>\n",
       "      <td>0.811445</td>\n",
       "      <td>0.810697</td>\n",
       "      <td>0.816445</td>\n",
       "      <td>0.811445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/miniconda3/envs/transformers/lib/python3.13/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/stefano/miniconda3/envs/transformers/lib/python3.13/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/stefano/miniconda3/envs/transformers/lib/python3.13/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/stefano/miniconda3/envs/transformers/lib/python3.13/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/stefano/miniconda3/envs/transformers/lib/python3.13/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/stefano/miniconda3/envs/transformers/lib/python3.13/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation metrics:\n",
      "eval_loss: 0.4060\n",
      "eval_accuracy: 0.8236\n",
      "eval_f1: 0.8236\n",
      "eval_precision: 0.8236\n",
      "eval_recall: 0.8236\n",
      "eval_runtime: 46.5326\n",
      "eval_samples_per_second: 22.9090\n",
      "eval_steps_per_second: 1.9130\n",
      "epoch: 7.0000\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "Test metrics:\n",
      "eval_loss: 0.4448\n",
      "eval_accuracy: 0.7871\n",
      "eval_f1: 0.7870\n",
      "eval_precision: 0.7872\n",
      "eval_recall: 0.7871\n",
      "eval_runtime: 46.6550\n",
      "eval_samples_per_second: 22.8490\n",
      "eval_steps_per_second: 1.9080\n",
      "epoch: 7.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "val_metrics = trainer.evaluate()\n",
    "print(\"\\nValidation metrics:\")\n",
    "for k, v in val_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_metrics = trainer.evaluate(tokenized_ds['test'])\n",
    "print(\"\\nTest metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8f9a1",
   "metadata": {},
   "source": [
    "**Analisi Risultati:**\n",
    "In questo esperimento si è adottato un metodo di fine-tuning più efficiente, grazie all'utilizzo di PEFT, riducendo il carico computazionale senza modificare drasticamente la pipeline.\n",
    "\n",
    "Dopo 7 epoche, l’accuratezza di validazione ha raggiunto un valore di 82.4%, comparabile al fine-tuning completo, ma con meno impatto computazionale. \n",
    "\n",
    "Le metriche stabili e bilanciate indicano una generalizzazione solida, anche se il test set mostra un calo a 78.7%, leggermente inferiore rispetto alla validazione. I tempi e le velocità di elaborazione restano nella norma, mostrando che l’efficienza è stata migliorata senza compromettere il throughput.\n",
    "\n",
    "In conclusione, il fine-tuning efficiente tramite tecniche come PEFT è una valida alternativa al full fine-tuning, mantenendo prestazioni competitive con meno risorse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "91b5ea8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./exercise_peft_lora/tokenizer_config.json',\n",
       " './exercise_peft_lora/special_tokens_map.json',\n",
       " './exercise_peft_lora/vocab.txt',\n",
       " './exercise_peft_lora/added_tokens.json',\n",
       " './exercise_peft_lora/tokenizer.json')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best model\n",
    "trainer.save_model('./exercise_peft_lora')\n",
    "\n",
    "# Save tokenizer as well\n",
    "tokenizer.save_pretrained('./exercise_peft_lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "09129254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: exposing the ways we fool ourselves is one hour photo's real strength .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: this kind of hands-on storytelling is ultimately what makes shanghai ghetto move beyond a good , dry , reliable textbook and what allows it to rank with its worthy predecessors .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: what's so striking about jolie's performance is that she never lets her character become a caricature -- not even with that radioactive hair .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n",
      "\n",
      "Text: you needn't be steeped in '50s sociology , pop culture or movie lore to appreciate the emotional depth of haynes' work . though haynes' style apes films from the period . . . its message is not rooted in that decade .\n",
      "True: positive\n",
      "Pred: positive\n",
      "---\n",
      "\n",
      "Text: works because we're never sure if ohlinger's on the level or merely a dying , delusional man trying to get into the history books before he croaks .\n",
      "True: positive\n",
      "Pred: negative\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for test set\n",
    "predictions = trainer.predict(tokenized_ds['test'])\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Show some examples\n",
    "sample_indices = [10, 20, 30, 40, 50]\n",
    "for idx in sample_indices:\n",
    "    text = dataset['test']['text'][idx]\n",
    "    true_label = dataset['test']['label'][idx]\n",
    "    pred_label = pred_labels[idx]\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"True: {'positive' if true_label else 'negative'}\")\n",
    "    print(f\"Pred: {'positive' if pred_label else 'negative'}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeca737-ee00-4d98-a3ae-f4d6eb3d264f",
   "metadata": {},
   "source": [
    "#### Exercise 3.2: Fine-tuning a CLIP Model (harder)\n",
    "\n",
    "Use a (small) CLIP model like [`openai/clip-vit-base-patch16`](https://huggingface.co/openai/clip-vit-base-patch16) and evaluate its zero-shot performance on a small image classification dataset like ImageNette or TinyImageNet. Fine-tune (using a parameter-efficient method!) the CLIP model to see how much improvement you can squeeze out of it.\n",
    "\n",
    "**Note**: There are several ways to adapt the CLIP model; you could fine-tune the image encoder, the text encoder, or both. Or, you could experiment with prompt learning.\n",
    "\n",
    "**Tip**: CLIP probably already works very well on ImageNet and ImageNet-like images. For extra fun, look for an image classification dataset with different image types (e.g. *sketches*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b59ec2-4fe6-44c6-ab0b-0069f486bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42ed48-444d-47d4-bd8b-839a99e7996a",
   "metadata": {},
   "source": [
    "#### Exercise 3.3: Choose your Own Adventure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f9129-ef2e-45f7-9e8f-baa697ccd91e",
   "metadata": {},
   "source": [
    "There are a *ton* of interesting and fun models on the HuggingFace hub. Pick one that does something interesting and adapt it in some way to a new task. Or, combine two or more models into something more interesting or fun. The sky's the limit.\n",
    "\n",
    "**Note**: Reach out to me by email or on the Discord if you are unsure about anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c150bd36-6535-4724-a06d-a61632d3132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
